\documentclass[11pt,letterpaper]{article}
\usepackage[margin=0.75in]{geometry}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{float}
\usepackage{enumitem}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{fancyvrb}
\usepackage{amsmath}
\usepackage{tcolorbox}

% Custom colors
\definecolor{codebg}{RGB}{245, 245, 245}
\definecolor{codegreen}{RGB}{34, 139, 34}
\definecolor{codeblue}{RGB}{0, 0, 255}
\definecolor{codered}{RGB}{220, 20, 60}

% Code listing settings
\lstdefinestyle{pythonstyle}{
    language=Python,
    basicstyle=\ttfamily\small,
    commentstyle=\color{codegreen}\itshape,
    keywordstyle=\color{codeblue}\bfseries,
    stringstyle=\color{codered},
    numberstyle=\tiny\color{gray},
    numbers=left,
    stepnumber=1,
    numbersep=8pt,
    breaklines=true,
    breakatwhitespace=false,
    showstringspaces=false,
    frame=single,
    rulecolor=\color{gray},
    backgroundcolor=\color{codebg},
    captionpos=b,
    xleftmargin=2em,
    framexleftmargin=1.5em
}

% Custom boxes for tips and warnings
\tcbuselibrary{skins,breakable}
\newtcolorbox{tipbox}{
    colback=blue!5!white,
    colframe=blue!75!black,
    fonttitle=\bfseries,
    title=TIP,
    breakable
}

\newtcolorbox{warningbox}{
    colback=red!5!white,
    colframe=red!75!black,
    fonttitle=\bfseries,
    title=WARNING,
    breakable
}

\newtcolorbox{notebox}{
    colback=green!5!white,
    colframe=green!75!black,
    fonttitle=\bfseries,
    title=NOTE,
    breakable
}

\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    pdftitle={HippoMedica How-To Guide},
}

\title{\textbf{How to Build a Multi-Modal Disease Detection System}
\large \\Using Machine Learning and SMOTE Balancing\\}
\author{HippoMedica Project}
\date{}

\begin{document}

\maketitle

\begin{abstract}
This comprehensive guide demonstrates how to build a production-ready AI system for medical disease detection. You'll learn advanced techniques including SMOTE class balancing with categorical post-processing, ensemble learning optimization, and web application deployment. The guide addresses real-world challenges in medical ML: severe class imbalance, categorical data corruption, and clinical validation. By following this step-by-step tutorial, you'll create a complete system capable of predicting diabetes, heart disease, and stroke with clinical-grade performance. Estimated completion time: 2-4 hours.
\end{abstract}

\newpage
\tableofcontents
\newpage

% ============================================================================
% INTRODUCTION
% ============================================================================

\section{Introduction}

\subsection{What You'll Learn}

This guide covers the complete workflow for building a medical AI system from scratch:

\begin{itemize}
    \item \textbf{Data Engineering}: Download and preprocess messy medical datasets with missing values and outliers
    \item \textbf{Advanced ML Techniques}: Implement SMOTE balancing with custom categorical post-processing
    \item \textbf{Ensemble Learning}: Train and compare Random Forest, XGBoost, and Neural Networks
    \item \textbf{Production Pipeline}: Create automated MLOps workflow with proper evaluation
    \item \textbf{Web Deployment}: Deploy models in interactive Streamlit web application
    \item \textbf{Troubleshooting}: Debug common medical ML problems with proven solutions
\end{itemize}

\subsection{Prerequisites and Setup}

\textbf{Required Knowledge}:
\begin{itemize}
    \item Python programming (intermediate level)
    \item Basic machine learning concepts (classification, training/testing, metrics)
    \item Familiarity with pandas and numpy
    \item Understanding of medical terminology helpful but not required
\end{itemize}

\textbf{Software Requirements}:
\begin{itemize}
    \item Python 3.8 or higher
    \item 8 GB RAM (16 GB recommended for SMOTE)
    \item 2 GB disk space for datasets and models
    \item Internet connection for dataset download
\end{itemize}

\textbf{Installation}:
\begin{lstlisting}[style=pythonstyle, caption=Environment Setup]
# Create virtual environment
python -m venv hippomedica_env

# Activate environment
# Windows:
hippomedica_env\Scripts\activate
# macOS/Linux:
source hippomedica_env/bin/activate

# Install dependencies
pip install pandas numpy scikit-learn xgboost imbalanced-learn
pip install matplotlib seaborn streamlit plotly joblib
\end{lstlisting}

\subsection{Project Structure}

\begin{verbatim}
HippoMedica/
|-- datasets/
|   |-- raw/              # Downloaded datasets
|   `-- processed/        # Cleaned and balanced data
|-- pipeline/
|   |-- 01_data_download.py
|   |-- 02_exploratory_analysis.py
|   |-- 03_data_preprocessing.py
|   |-- 04_model_training.py
|   `-- 05_model_storage.py
|-- models/
|   |-- trained_models/   # Serialized models (.pkl)
|   |-- metadata/         # Performance metrics (JSON)
|   `-- scalers/          # Feature scalers for NN
|-- web_app/
|   `-- app.py            # Streamlit web interface
`-- requirements.txt
\end{verbatim}

% ============================================================================
% STEP 1: DATA ACQUISITION
% ============================================================================

\section{Step 1: Dataset Acquisition and Exploration}

\subsection{Understanding the Datasets}

We'll work with three medical datasets, each presenting unique challenges:

\begin{table}[H]
\centering
\caption{Dataset Characteristics and Challenges}
\small
\begin{tabular}{@{}lcccp{5cm}@{}}
\toprule
\textbf{Dataset} & \textbf{Samples} & \textbf{Features} & \textbf{Imbalance} & \textbf{Key Challenge} \\
\midrule
Diabetes & 768 & 8 & 1.87:1 & Zero values represent missing data \\
Heart Disease & 303 & 13 & 1.18:1 & Small dataset limits deep learning \\
Stroke & 5,110 & 11 & 19.52:1 & Extreme imbalance causes 0\% recall \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Data Downloader Implementation}

The data downloader retrieves datasets from public repositories. The key concept is storing dataset metadata (URL, column names, whether headers exist) in a configuration dictionary, then using a generic download function that handles any dataset type.

Here's the core pattern using the Heart Disease dataset as an example:

\begin{lstlisting}[style=pythonstyle, caption=Data Download Pattern]
# Store dataset metadata in configuration
data_sources = {
    'heart_disease': {
        "url": "https://archive.ics.uci.edu/ml/.../processed.cleveland.data",
        "filename": "heart.csv",
        "columns": ['age', 'sex', 'cp', 'trestbps', 'chol', ...],
        "has_header": False
    }
}

# Download with requests library
response = requests.get(source['url'], timeout=30)

# Handle datasets without headers by manually assigning column names
df = pd.DataFrame(data_rows, columns=source['columns'])
df.to_csv(filepath, index=False)
\end{lstlisting}

The downloader also cleans data during import, replacing placeholder values like \texttt{'?'} with empty strings so they can be properly handled during preprocessing. This approach is reusable: simply add new entries to the \texttt{data\_sources} dictionary for additional datasets.

\begin{tipbox}
\textbf{Best Practice}: Always use \texttt{timeout=30} when downloading to prevent indefinite hanging if a server is unresponsive.
\end{tipbox}

\subsection{Exploratory Data Analysis}

Before preprocessing, always explore your data to understand its quality and structure. Key questions to answer:

\begin{itemize}
    \item What is the shape of the data (rows and columns)?
    \item Are there missing values or invalid entries?
    \item How are the classes distributed (balanced vs. imbalanced)?
    \item Which features correlate strongly with the target variable?
\end{itemize}

\textbf{Why Correlation Matters}: A correlation heatmap reveals relationships between features and the target. Features with high correlation to the outcome (like Glucose for diabetes) are likely strong predictors. Conversely, highly correlated features with each other may indicate redundancy and you might drop one to reduce overfitting.

\begin{lstlisting}[style=pythonstyle, caption=Key EDA Steps]
df = pd.read_csv(filepath)

# Check class distribution - critical for medical datasets
class_counts = df['Outcome'].value_counts()
imbalance_ratio = class_counts.max() / class_counts.min()

# Generate correlation heatmap
sns.heatmap(df.corr(), annot=True, cmap='coolwarm', center=0)
\end{lstlisting}

In our diabetes dataset, we found a 1.87:1 imbalance ratio, moderate but manageable. The stroke dataset, however, showed a severe 19.52:1 imbalance that required SMOTE intervention (covered in Step 2).

\begin{figure}[H]
\centering
\begin{subfigure}[b]{0.65\textwidth}
    \includegraphics[width=\textwidth]{figures/diabetes_correlation.png}
    \caption{Diabetes: Glucose shows strongest correlation}
\end{subfigure}

\vspace{0.5em}

\begin{subfigure}[b]{0.65\textwidth}
    \includegraphics[width=\textwidth]{figures/stroke_correlation.png}
    \caption{Stroke: Age emerges as key predictor}
\end{subfigure}
\caption{Correlation Heatmaps Reveal Feature Relationships: Darker colors indicate stronger correlations. Glucose and age emerge as primary disease predictors in their respective datasets.}
\label{fig:correlations}
\end{figure}

% ============================================================================
% STEP 2: DATA PREPROCESSING AND SMOTE
% ============================================================================

\section{Step 2: Advanced Data Preprocessing}

\subsection{Medical-Specific Data Cleaning}

Medical datasets often contain domain-specific issues that require careful handling. The two main challenges are:

\begin{enumerate}
    \item \textbf{Impossible Zero Values}: In the diabetes dataset, values like Glucose=0 or BloodPressure=0 are physiologically impossible. These represent missing data encoded as zeros, not actual measurements.
    \item \textbf{Categorical Encoding}: Text-based features like "gender" or "work\_type" must be converted to numbers for ML algorithms.
\end{enumerate}

Using diabetes as our example, here's how to handle these issues:

\begin{lstlisting}[style=pythonstyle, caption=Handling Missing Data in Diabetes Dataset]
# Define which columns cannot have zero values
zero_not_possible = ['Glucose', 'BloodPressure', 'SkinThickness', 
                     'Insulin', 'BMI']

# Remove rows where these columns have zeros
for col in zero_not_possible:
    df = df[df[col] != 0]
    
# Result: 768 rows -> 387 rows (removes ~50% of data)
\end{lstlisting}

For datasets with text categories (like the stroke dataset), use \texttt{LabelEncoder}:

\begin{lstlisting}[style=pythonstyle, caption=Encoding Categorical Features]
from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
df['gender'] = le.fit_transform(df['gender'])  # 'Male'->1, 'Female'->0
\end{lstlisting}

\begin{warningbox}
\textbf{Critical}: In medical datasets, zero values often represent missing data rather than actual measurements. Removing these rows prevents the model from learning that "glucose=0 means healthy," which would be clinically meaningless.
\end{warningbox}

\subsection{SMOTE Implementation with Categorical Correction}

Class imbalance is a critical problem in medical ML. For example, the stroke dataset has a 19.52:1 ratio (healthy to stroke cases). If you train on this data directly, the model achieves 95\% accuracy by \textit{never} predicting stroke, resulting in 0\% recall. This is clinically useless.

\textbf{SMOTE (Synthetic Minority Over-sampling Technique)} solves this by generating synthetic samples for the minority class. It works by interpolating between existing minority samples in feature space.

\begin{lstlisting}[style=pythonstyle, caption=Applying SMOTE]
from imblearn.over_sampling import SMOTE

# Check imbalance ratio first
imbalance_ratio = class_counts.max() / class_counts.min()

if imbalance_ratio > 3.0:
    smote = SMOTE(random_state=42, k_neighbors=5)
    X_balanced, y_balanced = smote.fit_resample(X, y)
    # Stroke: 5,110 samples -> 9,696 samples (1:1 ratio)
\end{lstlisting}

\textbf{The Categorical Problem}: SMOTE interpolates feature values, which creates invalid decimal values for categorical features (e.g., gender=0.613 instead of 0 or 1). The fix is simple, round and clip these values after SMOTE:

\begin{lstlisting}[style=pythonstyle, caption=Fixing Categorical Features After SMOTE]
# After SMOTE, fix categorical columns
for col in categorical_columns:
    X_balanced[col] = X_balanced[col].round().astype(int)
    X_balanced[col] = X_balanced[col].clip(
        original_min, original_max
    )
\end{lstlisting}

\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth]{figures/smote_comparison.png}
\caption{SMOTE Transformation Impact: Before SMOTE, the stroke dataset's 19.52:1 imbalance caused 0\% recall. After balancing to a 1:1 ratio, the model achieves 97.11\% recall.}
\label{fig:smote}
\end{figure}

\begin{notebox}
\textbf{Why This Works}: SMOTE creates synthetic minority samples by interpolating between existing ones. The categorical post-processing step rounds these interpolated values back to valid integers, preserving data integrity while maintaining the statistical benefits of balanced classes.
\end{notebox}

% ============================================================================
% STEP 3: MODEL TRAINING
% ============================================================================

\section{Step 3: Ensemble Model Training}

\subsection{Model Configuration}

We train three complementary algorithms, each with different strengths:

\begin{itemize}
    \item \textbf{Random Forest}: Robust, handles feature interactions well, built-in feature importance
    \item \textbf{XGBoost}: Excellent for structured data, handles class imbalance with \texttt{scale\_pos\_weight}
    \item \textbf{Neural Network (MLP)}: Can learn complex patterns, but requires feature scaling
\end{itemize}

Key hyperparameter choices explained:

\begin{lstlisting}[style=pythonstyle, caption=Model Configuration Highlights]
# Random Forest: use balanced weights for mild imbalance
rf = RandomForestClassifier(
    n_estimators=200,          # More trees = more stable predictions
    class_weight='balanced',   # Auto-adjusts for imbalanced classes
    n_jobs=-1                  # Use all CPU cores
)

# XGBoost: explicitly handle class imbalance
xgb = XGBClassifier(
    n_estimators=200,
    learning_rate=0.05,        # Low rate prevents overfitting
    scale_pos_weight=2.0       # Weight positive class 2x higher
)

# Neural Network: requires scaled features
nn = MLPClassifier(
    hidden_layer_sizes=(128, 64, 32),  # 3 hidden layers
    early_stopping=True                 # Prevents overfitting
)
\end{lstlisting}

\begin{tipbox}
\textbf{Important}: Neural networks are sensitive to feature magnitudes. Always use \texttt{StandardScaler} before training MLPClassifier tree-based methods (Random Forest, XGBoost) don't need scaling.
\end{tipbox}

\subsection{Training and Evaluation Pipeline}

The training workflow follows these steps: (1) split data into 80\% train / 20\% test with stratification to preserve class ratios, (2) train each model, (3) evaluate using multiple metrics.\\

For medical applications, \textbf{recall} is often more important than accuracy, i.e., we'd rather have false positives (unnecessary tests) than miss actual disease cases (false negatives).

\begin{lstlisting}[style=pythonstyle, caption=Core Evaluation Metrics]
# Train and predict
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
y_pred_proba = model.predict_proba(X_test)[:, 1]

# Calculate key metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)      # Critical for medical
f1 = f1_score(y_test, y_pred)
roc_auc = roc_auc_score(y_test, y_pred_proba)

# Cross-validation for robust estimates
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
cv_scores = cross_val_score(model, X_train, y_train, cv=cv)
\end{lstlisting}

After training, the pipeline outputs metrics for each model and identifies the best performer. In our experiments, Random Forest consistently achieved the highest accuracy (96.80\% on stroke), while XGBoost sometimes had slightly better recall.

\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth]{figures/performance_comparison.png}
\caption{Model Performance Comparison: Random Forest consistently outperforms other models across all diseases. Tree-based methods excel on small-to-medium medical datasets.}
\label{fig:performance}
\end{figure}

% ============================================================================
% STEP 4: WEB DEPLOYMENT
% ============================================================================

\section{Step 4: Web Application Deployment}

\subsection{Streamlit Interface Creation}

Streamlit makes it easy to create interactive web interfaces for ML models. The key components are:

\begin{enumerate}
    \item \textbf{Model Loading}: Use \texttt{@st.cache\_resource} to load models once and reuse them
    \item \textbf{Input Forms}: Create user-friendly forms for entering patient data
    \item \textbf{Prediction Display}: Show results with confidence scores and recommendations
\end{enumerate}

\begin{lstlisting}[style=pythonstyle, caption=Core Streamlit Pattern]
import streamlit as st
import joblib

@st.cache_resource
def load_models():
    """Load models once, cache for performance."""
    return joblib.load('models/diabetes_random_forest.pkl')

# Create input form
with st.form("diabetes_form"):
    glucose = st.number_input("Glucose (mg/dL)", 50, 300, 120)
    bmi = st.number_input("BMI", 10.0, 60.0, 25.0)
    age = st.number_input("Age", 18, 100, 30)
    # ... other inputs
    submitted = st.form_submit_button("Predict")

if submitted:
    # Make prediction
    input_data = np.array([[glucose, bmi, age, ...]])
    prediction = model.predict(input_data)[0]
    probability = model.predict_proba(input_data)[0]
    
    # Display result
    if prediction == 1:
        st.error(f"HIGH RISK - Probability: {probability[1]:.1%}")
    else:
        st.success(f"LOW RISK - Probability: {probability[1]:.1%}")
\end{lstlisting}

The application also includes input validation to warn users when values fall outside normal medical ranges (e.g., glucose below 70 or above 200 mg/dL). Results are displayed with clear recommendations and a confidence visualization.

\subsection{Running the Web Application}

Launch the app with a single command:

\begin{verbatim}
cd web_app
streamlit run app.py
# Opens at http://localhost:8501
\end{verbatim}

\begin{notebox}
\textbf{Deployment Options}: For production, consider Streamlit Cloud (free hosting), Heroku, or Docker containers. Ensure trained model files are included in the deployment package.
\end{notebox}

% ============================================================================
% STEP 5: TROUBLESHOOTING
% ============================================================================

\section{Step 5: Troubleshooting Common Issues}

\subsection{Issue 1: SMOTE Creates Invalid Categorical Values}

\textbf{Symptom}: After SMOTE, categorical features have decimal values (e.g., gender=0.613 instead of 0 or 1).

\textbf{Cause}: SMOTE interpolates between samples, creating fractional values for discrete features.

\textbf{Solution}: Round and clip categorical columns to their original valid range after applying SMOTE:

\begin{lstlisting}[style=pythonstyle]
for col in categorical_columns:
    X_balanced[col] = X_balanced[col].round().astype(int)
    X_balanced[col] = X_balanced[col].clip(0, 1)  # For binary
\end{lstlisting}

\subsection{Issue 2: Model Predicts Only Majority Class (0\% Recall)}

\textbf{Symptom}: High accuracy (95\%) but the model never predicts the minority class.

\textbf{Cause}: Severe class imbalance makes it "optimal" to always predict the majority class.

\textbf{Solutions}:
\begin{itemize}
    \item Apply SMOTE to balance classes before training
    \item Use \texttt{class\_weight='balanced'} in Random Forest
    \item Set \texttt{scale\_pos\_weight} in XGBoost equal to the imbalance ratio
\end{itemize}

\subsection{Issue 3: Neural Network Won't Converge}

\textbf{Symptom}: MLPClassifier shows "ConvergenceWarning: Maximum iterations reached."

\textbf{Solutions}:
\begin{itemize}
    \item Increase \texttt{max\_iter} (default 200 is often too low)
    \item Enable \texttt{early\_stopping=True} with \texttt{validation\_fraction=0.1}
    \item \textbf{Scale your features!} Neural networks require \texttt{StandardScaler}
\end{itemize}

\begin{warningbox}
\textbf{Critical}: ALWAYS scale features using \texttt{StandardScaler} before training neural networks. This is the most common cause of convergence issues.
\end{warningbox}

\subsection{Issue 4: Web App Model Loading Errors}

\textbf{Symptom}: Streamlit shows "FileNotFoundError" when trying to load models.

\textbf{Cause}: Models haven't been trained yet, or paths are incorrect.

\textbf{Solution}: Run the full pipeline first (\texttt{python run\_pipeline.py}), then verify model files exist in \texttt{models/trained\_models/}. Use \texttt{Path} for cross-platform path handling.

\subsection{Issue 5: Low Recall Despite Balanced Data}

\textbf{Symptom}: Even after SMOTE, test recall remains low (40-50\%).

\textbf{Solutions}:
\begin{itemize}
    \item Reduce model complexity (lower \texttt{max\_depth}, more \texttt{min\_samples\_split})
    \item Lower the decision threshold: \texttt{y\_pred = (proba >= 0.3).astype(int)}
    \item Use stratified cross-validation to ensure fair evaluation
\end{itemize}

% ============================================================================
% ADVANCED TECHNIQUES
% ============================================================================

\section{Advanced Techniques and Best Practices}

\subsection{Feature Importance Analysis}

Understanding which features drive predictions is crucial for medical ML as it validates that the model learned clinically meaningful patterns rather than spurious correlations.

Random Forest provides built-in feature importance scores via the Gini impurity reduction:

\begin{lstlisting}[style=pythonstyle, caption=Extract Feature Importance]
# Get importance from trained Random Forest
importances = model.feature_importances_
indices = np.argsort(importances)[::-1]

# Display top features
for i, idx in enumerate(indices[:5]):
    print(f"{i+1}. {feature_names[idx]}: {importances[idx]*100:.2f}%")
\end{lstlisting}

For our diabetes model, Glucose (25.78\%) emerged as the top predictor, followed by Insulin (17.16\%) and Age (15.41\%), exactly what medical literature would predict. This alignment with domain knowledge validates our model.

\begin{figure}[H]
\centering
\includegraphics[width=0.75\textwidth]{figures/feature_importance_diabetes.png}
\caption{Feature Importance for Diabetes: Glucose, Insulin, and Age are the top predictors, aligning with established medical knowledge.}
\label{fig:feature_importance}
\end{figure}

\subsection{Medical Range Validation}

Before making predictions, validate that input values fall within physiologically plausible ranges. This catches data entry errors and improves user trust:

\begin{lstlisting}[style=pythonstyle, caption=Input Validation Example]
validation_rules = {
    'Glucose': (70, 200, "Normal fasting: 70-100 mg/dL"),
    'BMI': (15, 50, "Normal: 18.5-24.9"),
    'Age': (18, 100, "Valid patient age")
}

for field, (min_val, max_val, desc) in validation_rules.items():
    if not (min_val <= value <= max_val):
        warnings.append(f"{field} outside expected range")
\end{lstlisting}

The web application displays these warnings to users, allowing them to correct potential errors before getting a prediction.

% ============================================================================
% CONCLUSION
% ============================================================================

\section{Conclusion}

\subsection{Summary}

This guide walked you through building a complete medical ML pipeline:

\begin{enumerate}
    \item \textbf{Data Acquisition}: Downloaded and explored three medical datasets
    \item \textbf{Preprocessing}: Handled missing values, encoded categories, applied SMOTE for class balance
    \item \textbf{Model Training}: Compared Random Forest, XGBoost, and Neural Networks
    \item \textbf{Deployment}: Built a Streamlit web app for real-time predictions
\end{enumerate}

\subsection{Key Takeaways}

\begin{itemize}
    \item \textbf{Class imbalance is critical}: Without SMOTE, models achieve high accuracy by ignoring the minority class entirely
    \item \textbf{SMOTE needs categorical post-processing}: Round interpolated categorical values back to valid integers
    \item \textbf{Tree-based methods excel on tabular data}: Random Forest outperformed Neural Networks on all three datasets
    \item \textbf{Feature importance validates models}: When top features align with medical knowledge, you know the model learned meaningful patterns
\end{itemize}

\begin{warningbox}
\textbf{Important}: This system is for educational purposes only. Medical AI systems require regulatory approval, clinical validation, and physician oversight before real-world deployment.
\end{warningbox}

\subsection{Resources}

\begin{itemize}
    \item scikit-learn: \url{https://scikit-learn.org/}
    \item imbalanced-learn (SMOTE): \url{https://imbalanced-learn.org/}
    \item Streamlit: \url{https://docs.streamlit.io/}
    \item XGBoost: \url{https://xgboost.readthedocs.io/}
\end{itemize}

The complete source code is available in the HippoMedica repository.

\end{document}
